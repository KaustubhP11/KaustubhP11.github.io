<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> StructFormer | Kaustubh S. Ponkshe </title> <meta name="author" content="Kaustubh S. Ponkshe"> <meta name="description" content="Document Structure-Based Pre-training"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kaustubhp11.github.io/projects/4_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kaustubh</span> S. Ponkshe </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">StructFormer</h1> <p class="post-description">Document Structure-Based Pre-training</p> </header> <article> <h2 id="abstract">Abstract</h2> <p>Most state-of-the-art techniques for Language Models (LMs) today rely on transformer-based architectures and their ubiquitous attention mechanism. However, the exponential growth in computational requirements with longer input sequences confines Transformers to handling short passages. Recent efforts have aimed to address this limitation by introducing selective attention mechanisms, notably local and global attention. While sparse attention mechanisms, akin to full attention in being Turing-complete, have been theoretically established, their practical impact on pre-training remains unexplored. This study focuses on empirically assessing the influence of global attention on BERT pre-training. The primary steps involve creating an extensive corpus of structure-aware text through arXiv data, alongside a text-only counterpart. We carry out pre-training on these two datasets, investigate shifts in attention patterns, and assess their implications for downstream tasks. Our analysis underscores the significance of incorporating document structure into LM models, demonstrating their capacity to excel in more abstract tasks, such as document understanding.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Structformer-overview-pic-480.webp 480w,/assets/img/Structformer-overview-pic-800.webp 800w,/assets/img/Structformer-overview-pic-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Structformer-overview-pic.jpg" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" style=" max-width: 600px; " title="StructFormer Overview" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of StructFormer showing our approach to empirically analyzing masked attention during pre-training. We process arXiv documents to create parallel corpora - one with only text and another that is structure-aware. These are used to pre-train and analyze models, evaluating their performance on downstream tasks. </div> <p>StructFormer introduces a novel approach to help language models better understand document structure during pre-training. By incorporating document headers as global attention tokens, the model learns to recognize and utilize document organization for improved comprehension.</p> <h2 id="the-challenge">The Challenge</h2> <p>Modern language models face two key limitations when processing long documents:</p> <ol> <li>Computational constraints with increasing sequence length</li> <li>Limited understanding of document structure and organization</li> </ol> <p>While sparse attention mechanisms help address the first issue, existing approaches typically ignore document structure during pre-training, only utilizing it during fine-tuning.</p> <h2 id="our-approach--results">Our Approach &amp; Results</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Final_att-480.webp 480w,/assets/img/Final_att-800.webp 800w,/assets/img/Final_att-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Final_att.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" style=" max-width: 600px; " title="Attention Patterns" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Attention patterns comparing structure-aware pre-training (left) versus vanilla pre-training (right) between header and keywords. The structure-aware model shows stronger attention relationships between document headers and relevant content. </div> <p>StructFormer modifies the pre-training process to make models inherently structure-aware through:</p> <ul> <li>Document headers as global attention tokens during pre-training</li> <li>Combined local and global attention mechanisms</li> <li>Structure-aware training data from LaTeX documents</li> <li>Sparse attention patterns for efficiency</li> </ul> <h2 id="results--analysis">Results &amp; Analysis</h2> <h3 id="scirex-performance-end-to-end-predicted-input">SciREX Performance (End-to-end predicted input)</h3> <table> <thead> <tr> <th>Task</th> <th>Model</th> <th>Precision</th> <th>Recall</th> <th>F1</th> </tr> </thead> <tbody> <tr> <td>Salient Clusters</td> <td>StructFormer</td> <td>0.2581</td> <td>0.6127</td> <td>0.3419</td> </tr> <tr> <td> </td> <td>SciREX Baseline</td> <td>0.2230</td> <td>0.6000</td> <td>0.3070</td> </tr> <tr> <td>Binary Relations</td> <td>StructFormer</td> <td>0.0550</td> <td>0.5100</td> <td>0.0890</td> </tr> <tr> <td> </td> <td>SciREX Baseline</td> <td>0.0650</td> <td>0.4110</td> <td>0.0960</td> </tr> <tr> <td>4-ary Relations</td> <td>StructFormer</td> <td>0.0019</td> <td>0.2760</td> <td>0.0037</td> </tr> <tr> <td> </td> <td>SciREX Baseline</td> <td>0.0070</td> <td>0.1730</td> <td>0.0080</td> </tr> </tbody> </table> <h3 id="glue-benchmark-results">GLUE Benchmark Results</h3> <table> <thead> <tr> <th>Task</th> <th>Metric</th> <th>Vanilla Longformer</th> <th>StructFormer</th> <th>BERT base</th> </tr> </thead> <tbody> <tr> <td>CoLA</td> <td>Mathews Correlation</td> <td>0.502</td> <td>0.469</td> <td>0.521</td> </tr> <tr> <td>STSB</td> <td>Combined Score</td> <td>0.871</td> <td>0.856</td> <td>0.858</td> </tr> <tr> <td>MRPC</td> <td>F1</td> <td>0.904</td> <td>0.927</td> <td>0.900</td> </tr> <tr> <td>QNLI</td> <td>Accuracy</td> <td>0.908</td> <td>0.910</td> <td>0.905</td> </tr> <tr> <td>SST2</td> <td>Accuracy</td> <td>0.928</td> <td>0.933</td> <td>0.935</td> </tr> <tr> <td>MNLI</td> <td>Accuracy</td> <td>0.867</td> <td>0.870</td> <td>0.833</td> </tr> </tbody> </table> <p>The results demonstrate StructFormer’s superior performance in document understanding tasks, particularly in salient cluster identification where it outperforms the baseline by ~3.5 percentage points. On GLUE tasks, StructFormer maintains competitive performance with BERT base while excelling in tasks like MRPC and QNLI, showing its structure-awareness doesn’t compromise general language understanding capabilities.</p> <h2 id="pipeline-implementation">Pipeline Implementation</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SciREX-predicted-480.webp 480w,/assets/img/SciREX-predicted-800.webp 800w,/assets/img/SciREX-predicted-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/SciREX-predicted.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" style=" max-width: 600px; " title="SciREX Pipeline" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> SciREX pipeline integrated with our structure-aware corpus pre-trained Longformer, showing the complete workflow from input processing to final predictions. </div> <p>The implementation involves:</p> <ol> <li>Processing 1.1M+ LaTeX documents from arXiv</li> <li>Combining local windowed attention with global attention for headers</li> <li>Using headers as global attention tokens during pre-training</li> <li>Fine-tuning on downstream tasks while maintaining structural awareness</li> </ol> <h2 id="resources">Resources</h2> <ul> <li><a href="https://arxiv.org/abs/2411.16618" rel="external nofollow noopener" target="_blank">Paper</a></li> <li><a href="https://github.com/KaustubhP11/StructFormer" rel="external nofollow noopener" target="_blank">GitHub Repository</a></li> <li><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Dataset Information</a></li> </ul> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Kaustubh S. Ponkshe. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>